{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E08w9hc0TAbI"
   },
   "source": [
    "# BioMedBERT: BREATHE -> (BERT+BioBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uz-18dZ2tDH"
   },
   "source": [
    "Save model assets and checkpoints to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMZKDdyL1pb3"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"ekaba-assets\"\n",
    "MODEL_DIR = \"biomedbert_base\"\n",
    "tf.io.gfile.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mVR9qBK3V5j"
   },
   "source": [
    "Hyparameter configuration for BERT BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC_SIZE = 32000\n",
    "# VOC_FNAME = \"biomedbert-8M.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIibj7MY3TH5"
   },
   "outputs": [],
   "source": [
    "# # use this for BERT-base\n",
    "\n",
    "# bert_base_config = {\n",
    "#     \"attention_probs_dropout_prob\": 0.1,\n",
    "#     \"directionality\": \"bidi\",\n",
    "#     \"hidden_act\": \"gelu\",\n",
    "#     \"hidden_dropout_prob\": 0.1,\n",
    "#     \"hidden_size\": 768,\n",
    "#     \"initializer_range\": 0.02,\n",
    "#     \"intermediate_size\": 3072,\n",
    "#     \"max_position_embeddings\": 512,\n",
    "#     \"num_attention_heads\": 12,\n",
    "#     \"num_hidden_layers\": 12,\n",
    "#     \"pooler_fc_size\": 768,\n",
    "#     \"pooler_num_attention_heads\": 12,\n",
    "#     \"pooler_num_fc_layers\": 3,\n",
    "#     \"pooler_size_per_head\": 128,\n",
    "#     \"pooler_type\": \"first_token_transform\",\n",
    "#     \"type_vocab_size\": 2,\n",
    "#     \"vocab_size\": VOC_SIZE\n",
    "# }\n",
    "\n",
    "# with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
    "#     json.dump(bert_base_config, fo, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update vocab\n",
    "# !cp ../vocabulary/full_text/biomedbert-8M.txt biomedbert_base/biomedbert-8M.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2QHoAJs63PrO",
    "outputId": "c98a8bcb-9ba0-43d1-9fe7-4df5f80419df"
   },
   "outputs": [],
   "source": [
    "# # move to GCS\n",
    "# !gsutil -m cp -r $MODEL_DIR gs://ekaba-assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bert modules\n",
    "sys.path.append(\"bert\")\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR5ffWV15OHf"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# configure logging\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "u0fRU1XT3vu-",
    "outputId": "268c16ab-9274-4d9a-e185-b46b556dfd00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using checkpoint: gs://ekaba-assets/biomedbert_base/model.ckpt-20577500\n",
      "INFO:tensorflow:Using 424 data shards\n"
     ]
    }
   ],
   "source": [
    "PRETRAINING_DIR = \"pre_trained_data_full_biomed\"\n",
    "VOC_FNAME = \"biomedbert-8M.txt\"\n",
    "\n",
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 128  # 128 -> 12.8K -> 1.2K\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 128  # 64, 128 - 12.8K -> 1.2K\n",
    "LEARNING_RATE = 1e-5  # 2e-5\n",
    "TRAIN_STEPS = 100000000  # 1M -> 100M\n",
    "SAVE_CHECKPOINTS_STEPS = 25000  # 2500 -> 25K\n",
    "NUM_TPU_CORES = 128\n",
    "\n",
    "if BUCKET_NAME:\n",
    "    BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
    "else:\n",
    "    BUCKET_PATH = \".\"\n",
    "\n",
    "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
    "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
    "\n",
    "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
    "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "# 'gs://ekaba-assets/biomedbert_base/model.ckpt-20577500\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.io.gfile.glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
    "log.info(\"Using {} data shards\".format(len(input_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEawhTlo5frp"
   },
   "source": [
    "**Train on TPUs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomedbert-preempt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export TPU_NAME='biomedbert-preempt'\n",
    "echo $TPU_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "aM4Vn5RZ3pqk",
    "outputId": "b96576de-fb9c-47a2-b770-572e99dba93b"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "TPU \"biomedbert-preempt\" is not yet ready; state: \"PREEMPTED\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f8d19e2de840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0miterations_per_loop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAVE_CHECKPOINTS_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mnum_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TPU_CORES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         per_host_input_for_training=tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2))\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_config.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tpu_config, evaluation_master, master, cluster, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevaluation_master\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/cluster_resolver/tpu_cluster_resolver.py\u001b[0m in \u001b[0;36mmaster\u001b[0;34m(self, task_type, task_id, rpc_layer)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;31m# We are going to communicate with the Cloud TPU APIs to get a Cluster.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m       \u001b[0mcluster_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtask_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# task_type and task_id is from the function parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/cluster_resolver/tpu_cluster_resolver.py\u001b[0m in \u001b[0;36mcluster_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'state'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'READY'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         raise RuntimeError('TPU \"%s\" is not yet ready; state: \"%s\"' %\n\u001b[0;32m--> 432\u001b[0;31m                            (compat.as_text(self._tpu), response['state']))\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'networkEndpoints'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TPU \"biomedbert-preempt\" is not yet ready; state: \"PREEMPTED\""
     ]
    }
   ],
   "source": [
    "USE_TPU = True\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10, #10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True\n",
    ")\n",
    "\n",
    "tpu_cluster_resolver =  tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "    zone='europe-west4-a', project='ai-vs-covid19', job_name='biomedbert', tpu='biomedbert-preempt')\n",
    "\n",
    "run_config = tf.compat.v1.estimator.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=BERT_GCS_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n",
    "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  \n",
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True,\n",
    "        num_cpu_threads=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-3KYolcy5kvn",
    "outputId": "03a57f14-eba6-4e78-9325-63677b8cb253"
   },
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BioMedBERT-Data-Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
