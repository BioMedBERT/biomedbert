{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extract-BioMedBERT-LG-Embedding-bioasq-8b.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VfFsHIUx_HXx"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aivscovid19/covid-19_research_collaboration/blob/master/notebooks/Extract_BioMedBERT_LG_Embedding_bioasq_8b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxbM9_dWmq59",
        "colab_type": "text"
      },
      "source": [
        "# MRR for BERT and BERT + RESCORE\n",
        "\n",
        "[Evaluation of Ranking for Medical Papers](https://github.com/aivscovid19/covid-19_research_collaboration/issues/9)\n",
        "\n",
        "This notebook shows how to perform 3 Experiments (as of May 24 2020) on ranking using the dataset from Bioasq.\n",
        "\n",
        "- Experiment 1: score: 0.9124373431039607\n",
        "    This uses the vanilla elastic search ranking algorithm \n",
        "\n",
        "- Experiment 2: BioMedBERT LG @ 770K train step: 0.5496368744211725\n",
        "\n",
        "- Experiment 3: 0.38342058777179133 using [universal-sentence-encoder-lite](https://tfhub.dev/google/universal-sentence-encoder-lite/2)\n",
        "\n",
        "\n",
        "Right now Elastic search algorithm is the best.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0iMu8hd-kIc",
        "colab_type": "text"
      },
      "source": [
        "# Setup Elastic Search Account\n",
        "\n",
        "Login to [Kibana](https://d7e0e807713c441295ed9707b13a089f.us-central1.gcp.cloud.es.io:9243/app/kibana#/dev_tools/console)\n",
        "Create a access token than expries in 30 days\n",
        "\n",
        "```\n",
        "POST /_security/api_key\n",
        "{\n",
        "  \"name\": \"suzanne\",\n",
        "  \"expiration\": \"30d\", \n",
        "  \"role_descriptors\": { \n",
        "    \"role-a\": {\n",
        "      \"cluster\": [\"all\"],\n",
        "      \"index\": [\n",
        "        {\n",
        "          \"names\": [\"bioasq-8b-baseline\", \"ncbi\"],\n",
        "          \"privileges\": [\"read\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_w6n4Ff_miS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# paste the token here\n",
        "ES_ACCOUNT = {\n",
        "  \"id\" : \"wpxtRXIBEis4ExSHjqqf\",\n",
        "  \"name\" : \"bert_embedding\",\n",
        "  \"expiration\" : 1590389107342,\n",
        "  \"api_key\" : \"2Sc8FRMnQ8i1kxJ4Dovs6Q\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfFsHIUx_HXx",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHS8WEWgTsfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.auth import authenticate_user\n",
        "authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M25dfqs81Un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWB-_4sg1ni5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --quiet elasticsearch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meQeF0pr850p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucrme9vJ9mrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from elasticsearch import helpers, Elasticsearch\n",
        "import csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUscCJMsSlTA",
        "colab_type": "text"
      },
      "source": [
        "# Read BIOASQ 8b question json  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAIELMffTGhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://bioasq/DATA/training8b_list.json ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFquycc1T6wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0k-xayHT3Aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('./training8b_list.json') as fd:\n",
        "    dataset = json.load(fd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8HboFAJUAtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head training8b_list.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx3qzLjAT_k5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions = dataset['questions']\n",
        "len(dataset['questions'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL-SbYpj_K4w",
        "colab_type": "text"
      },
      "source": [
        "# Upload to Elastic Search "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NS9Vg0W1v0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ES_ENDPOINT='https://d1f43211bd5c4fc29e56a232832b7b17.us-central1.gcp.cloud.es.io:9243'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqdGUKfZ2wPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAkEljHg1tYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = Elasticsearch([ES_ENDPOINT], api_key=(ES_ACCOUNT['id'], ES_ACCOUNT['api_key']))\n",
        "es.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6O8Hf8Q3XKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SEARCH(text, index, key, limit=31):\n",
        "    res = es.search(index=index,\n",
        "                    body={\n",
        "                        \"query\": {\n",
        "                            \"match\": {\n",
        "                                'context': {\n",
        "                                    \"query\": text,\n",
        "                                    \"operator\": \"or\",\n",
        "                                    \"fuzziness\": \"0\"\n",
        "                                }\n",
        "                            }\n",
        "                        },\n",
        "                        \"min_score\": -1,\n",
        "                    },\n",
        "                    size=limit)\n",
        "\n",
        "    return ([(x.get('_source'), x.get('_score')) for x in res['hits']['hits']])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtoHKzwRZ3nS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qin = questions[0]['question']\n",
        "qin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ADyCJyE342f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = SEARCH(qin, 'bioasq-8b-baseline', 'context', limit=700)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKhWO9TDc5TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzaojXm3dGDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6njeu7vxULXJ",
        "colab_type": "text"
      },
      "source": [
        "# Create BERT embeddings from the BIOSEQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqJzF-j86mWw",
        "colab_type": "text"
      },
      "source": [
        "## Load BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Sf5zZU6uuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNdrX9Nx6la-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bucket_name = 'ekaba-assets'\n",
        "model_dir = 'COPY_biomedbert_base_bert_weights_and_vocab'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci3jW3m88klm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "![ -d bert ] || git clone https://github.com/google-research/bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IfWZO3lDP17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlxwTN6RUdnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title get sent embedding\n",
        "def get_sent_embed(dict_list) :\n",
        "    sent_embed = []\n",
        "    for line in dict_list:\n",
        "        feat_embed = np.array(line['features'][0]['layers'][0]['values'])\n",
        "        # assert feat_embed.sum() > 0\n",
        "        sent_embed.append(feat_embed)\n",
        "    return sent_embed\n",
        "     \n",
        "def cosine_rank(qe, ce) :\n",
        "    # ce (644, 1024)\n",
        "    \n",
        "    # Query Embedding\n",
        "    query_embed_norm_np = query_embed/np.norm(query_embed)\n",
        "    \n",
        "    # Abstracts embedding\n",
        "    abstract_list_embed_py = df_abstracts['embedding'].tolist()\n",
        "    \n",
        "    # getting the Abstracts Embedding\n",
        "    abstract_list_embed_np =  np.stack(abstract_list_embed_py, \n",
        "                                       axis=0)\n",
        "    abstract_list_embed_norm_np = abstract_list_embed_np/norm(abstract_list_embed_np, \n",
        "                                         axis=1, keepdims= True)\n",
        "    \n",
        "    # compute the cosine similarity\n",
        "    # shape: ( len(ABSTRACTS), 1)\n",
        "    cos_sim = np.dot(abstract_list_embed_norm_np, \n",
        "                    query_embed_norm_np)\n",
        "    rank_index = cos_sim.argsort()[::-1]\n",
        "    scores_sorted = np.sort(cos_sim)[::-1]\n",
        "\n",
        "    return rank_index, scores_sorted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjf2ZmF2QKu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/bert')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po2H7pyfRMds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import modeling\n",
        "import tokenization\n",
        "import tensorflow as tf\n",
        "\n",
        "from extract_features import InputExample\n",
        "\n",
        "def read_examples(text_lines=[]):\n",
        "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
        "    examples = []\n",
        "    unique_id = 0\n",
        "    for line in text_lines:\n",
        "        line = line.strip()\n",
        "        linet = tokenization.convert_to_unicode(line)\n",
        "        text_a = None\n",
        "        text_b = None\n",
        "        m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
        "        if m is None:\n",
        "            text_a = line\n",
        "        else:\n",
        "            text_a = m.group(1)\n",
        "            text_b = m.group(2)\n",
        "        examples.append(\n",
        "            InputExample(unique_id=unique_id, \n",
        "                            text_a=text_a, \n",
        "                            text_b=text_b))\n",
        "        unique_id += 1\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJVmT0VbaRtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder():\n",
        "  all_unique_ids = []\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_input_type_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_unique_ids.append(feature.unique_id)\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_input_type_ids.append(feature.input_type_ids)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"unique_ids\":\n",
        "            tf.constant(all_unique_ids, shape=[num_examples], dtype=tf.int32),\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_type_ids\":\n",
        "            tf.constant(\n",
        "                all_input_type_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=False)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6NrWMdaP_2o",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "from extract_features import convert_examples_to_features, model_fn_builder, input_fn_builder\n",
        "\n",
        "import collections\n",
        "\n",
        "def load_model(model_path, batch_size=8):\n",
        "\n",
        "    init_checkpoint = tf.train.latest_checkpoint(model_path)\n",
        "    vocab_file = os.path.join(model_path, 'vocab.txt')\n",
        "    bert_config_file = os.path.join(model_path, 'bert_config.json')\n",
        "    # TODO: assert the files exists\n",
        "\n",
        "    # we only pick the last layer\n",
        "    layer_indexes = [-1]\n",
        "    \n",
        "    bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
        "   \n",
        "    do_lower_case = True\n",
        "\n",
        "    tokenizer = tokenization.FullTokenizer(\n",
        "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "    \n",
        "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "    master = None\n",
        "    num_tpu_cores = 8\n",
        "    run_config = tf.contrib.tpu.RunConfig(\n",
        "        master=master,\n",
        "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "            num_shards=num_tpu_cores,\n",
        "            per_host_input_for_training=is_per_host))\n",
        "   \n",
        "    use_tpu = False\n",
        "    use_one_hot_embeddings = False\n",
        "    # BERT MODEL\n",
        "    model_fn = model_fn_builder(\n",
        "        bert_config=bert_config,\n",
        "        init_checkpoint=init_checkpoint,\n",
        "        layer_indexes=layer_indexes,\n",
        "        use_tpu=use_tpu,\n",
        "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "    \n",
        "    # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "    # or GPU.\n",
        "    estimator = tf.contrib.tpu.TPUEstimator(\n",
        "        use_tpu=use_tpu,\n",
        "        model_fn=model_fn,\n",
        "        config=run_config,\n",
        "        predict_batch_size=batch_size)\n",
        "    \n",
        "    return estimator, tokenizer\n",
        "\n",
        "\n",
        "def predict_fn(input_values, estimator, tokenizer, max_seq_length=128):\n",
        "    # process input \n",
        "    examples = read_examples(input_values)\n",
        "\n",
        "    # Build \n",
        "    features = convert_examples_to_features(\n",
        "        examples=examples, seq_length=max_seq_length, tokenizer=tokenizer)\n",
        "    \n",
        "    # input_fn = input_fn_builder(\n",
        "    #     features=features, seq_length=max_seq_length)\n",
        "    \n",
        "    all_unique_ids = []\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_input_type_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_unique_ids.append(feature.unique_id)\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_input_type_ids.append(feature.input_type_ids)\n",
        "\n",
        "    def input_fn(params):\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "        batch_size = params[\"batch_size\"]\n",
        "    \n",
        "        num_examples = len(features)\n",
        "    \n",
        "        # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "        # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "        # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "        d = tf.data.Dataset.from_tensor_slices({\n",
        "            \"unique_ids\":\n",
        "                tf.constant(all_unique_ids, shape=[num_examples], dtype=tf.int32),\n",
        "            \"input_ids\":\n",
        "                tf.constant(\n",
        "                    all_input_ids, shape=[num_examples, max_seq_length],\n",
        "                    dtype=tf.int32),\n",
        "            \"input_mask\":\n",
        "                tf.constant(\n",
        "                    all_input_mask,\n",
        "                    shape=[num_examples, max_seq_length],\n",
        "                    dtype=tf.int32),\n",
        "            \"input_type_ids\":\n",
        "                tf.constant(\n",
        "                    all_input_type_ids,\n",
        "                    shape=[num_examples, max_seq_length],\n",
        "                    dtype=tf.int32),\n",
        "        })\n",
        "\n",
        "        d = d.batch(batch_size=batch_size)\n",
        "        return d\n",
        "\n",
        "    unique_id_to_feature = {}\n",
        "    for feature in features:\n",
        "        unique_id_to_feature[feature.unique_id] = feature\n",
        "\n",
        "    layer_indexes = [-1]\n",
        "    result_list = []\n",
        "    for result in estimator.predict(input_fn, yield_single_examples=True):\n",
        "        unique_id = int(result[\"unique_id\"])\n",
        "        feature = unique_id_to_feature[unique_id]\n",
        "        output_json = collections.OrderedDict()\n",
        "        output_json[\"linex_index\"] = unique_id\n",
        "        all_features = []\n",
        "        for (i, token) in enumerate(feature.tokens):\n",
        "            all_layers = []\n",
        "            for (j, layer_index) in enumerate(layer_indexes):\n",
        "                layer_output = result[\"layer_output_%d\" % j]\n",
        "                layers = collections.OrderedDict()\n",
        "                layers[\"index\"] = layer_index\n",
        "                layers[\"values\"] = [\n",
        "                    round(float(x), 6) for x in layer_output[i:(i + 1)].flat\n",
        "                ]\n",
        "                all_layers.append(layers)\n",
        "            features = collections.OrderedDict()\n",
        "            features[\"token\"] = token\n",
        "            features[\"layers\"] = all_layers\n",
        "            all_features.append(features)\n",
        "        output_json[\"features\"] = all_features\n",
        "        result_list.append(output_json)\n",
        "    return result_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HFIpRYeAJ5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_LOCATION=f'gs://{bucket_name}/{model_dir}/'\n",
        "MODEL_LOCATION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joHeQfLqYHg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator, tokenizer = load_model(MODEL_LOCATION, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW5FBmo7V8Vv",
        "colab_type": "text"
      },
      "source": [
        "# QUERY SYSTEM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2toPXqc2YAB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_embeddings(questions):\n",
        "    # 1. extract context\n",
        "    context_list = [ q['context'] for q in questions]\n",
        "    query_list = [ q['question'] for q in questions] \n",
        "    # 2. compute the embeddings\n",
        "    embedding_list = predict_fn(query_list + context_list, estimator, tokenizer)\n",
        "\n",
        "    query_embeddings = embedding_list[:len(query_list)] \n",
        "    contex_embeddings = embedding_list[len(query_list):] \n",
        "    assert len(query_embeddings) == len(context_embeddings)\n",
        "\n",
        "    query_embeddings = get_sent_embed(query_embeddings)\n",
        "    context_embeddings = get_sent_embed(context_embeddings)\n",
        "    \n",
        "    return query_embeddings, context_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFzUJENSYk1d",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def do_query(query):\n",
        "    # 1. Ask elastic search for the data\n",
        "    results = SEARCH(query, 'ncbi', 'title', limit=100)\n",
        "    input_values = [query]\n",
        "    for doc, score in results:\n",
        "        if 'abstract' in doc:\n",
        "            abstract = doc['abstract']\n",
        "            input_values.append(abstract)\n",
        "\n",
        "    print(64, len(input_values))\n",
        "    input_values = input_values[:64]\n",
        "    # 2. Compute Embeddings for query + abstracts\n",
        "    bert_output_list = predict_fn(input_values, estimator, tokenizer)\n",
        "    # 3. Rank them\n",
        "    abstracts_text_list = input_values[1:]\n",
        "    rank_index, scores = rank_abstracts(bert_output_list)\n",
        "    df = pd.DataFrame({ 'abstract': abstracts_text_list, \"original_rank\": rank_index, \"scores\": scores }, index=rank_index) \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmyPjNw1Brpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query=\"What are the COVID-19 symptoms?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEvXl6V3Ye5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "query_result = do_query(query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q3Rzui3ugE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zP8d3BEr7Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cphoNIthWKWq",
        "colab_type": "text"
      },
      "source": [
        "# Compute MRR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEPFCkhfWp7M",
        "colab_type": "text"
      },
      "source": [
        "The '''mean reciprocal rank''' is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness.\n",
        "\n",
        "The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: \n",
        "- 1 for first place, \n",
        "- $\\frac12$ for second place, \n",
        "- $\\frac13$ for third place and so on.\n",
        "\n",
        "The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries \n",
        "\n",
        "$$\n",
        " \\text{MRR} = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{\\text{rank}_i}\n",
        "$$\n",
        "\n",
        "where <math> \\text{rank}_i</math> refers to the rank position of the ''first'' relevant document for the ''i''-th query.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmWryx1Xcebm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmaUItvYgt-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textwrap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDXGFrBJiKmn",
        "colab_type": "text"
      },
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvObaQY_brX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = []\n",
        "all_results = []\n",
        "#enumerate(questions[:10])\n",
        "it = tqdm(enumerate(questions), total=len(questions))\n",
        "\n",
        "for qix, q in it:\n",
        "    qin = q['question']\n",
        "    ranking = SEARCH(qin, 'bioasq-8b-baseline', 'context', limit=700)\n",
        "    all_results.append(ranking)\n",
        "    for i, (r, _) in enumerate(ranking):\n",
        "        #print('\\n\\n', q['question'], '\\n\\n\\n',  '\\n'.join(textwrap.wrap(r['context'], 80)))\n",
        "        if r['context'] == q['context']:\n",
        "            scores.append(1./(i+1.)) \n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpvKaTDmfRR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_QO27Jbh0jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19FoS_1JiOmF",
        "colab_type": "text"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK4cz-Atn0QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. extract context\n",
        "context_list = [ q['context'] for q in questions]\n",
        "query_list = [ q['question'] for q in questions] \n",
        "# 2. compute the embeddings\n",
        "embedding_list = predict_fn(query_list + context_list, estimator, tokenizer)\n",
        "\n",
        "query_embeddings = embedding_list[:len(query_list)] \n",
        "context_embeddings = embedding_list[len(query_list):] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvifJQ86pac6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert len(query_embeddings) == len(context_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ0spGmWn6qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_embeddings_x = get_sent_embed(query_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2EHt8PgpiRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_embeddings_x = get_sent_embed(context_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Cb7VA3pp4kW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "QE = np.array(query_embeddings_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDdbUb60qCNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CE = np.array(context_embeddings_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDOf9eAwqEuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "QE.shape, CE.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEFkcl8_qOml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0iK7qI6oics",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = cosine_similarity(QE, CE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5md82ORqSdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoVwXaaUqsgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_similarity(CE, CE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIQqKJTAqaTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_similarity(QE, QE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnq0crLgrhBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qix = 100\n",
        "qe = QE[qix]\n",
        "questions[qix]['question'], questions[qix]['context'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1i0LyNovGjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CE.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnZWKfXvJ2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CES= CE[:100,]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqWSooj1vO3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = cosine_similarity(QE, CES)\n",
        "scores.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsrWBE8Drl_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = cosine_similarity([qe], CE)\n",
        "scores.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k6ASaMirvYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ranking = scores[0].argsort() # [::-1]\n",
        "ranking.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj9h_WpKwZ3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ranking[rix]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L77aFQZwbek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_-gjelJsByD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_ix = np.where(ranking == qix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec6DHGQGsOdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rix = result_ix[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUazbOvpvuGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions[rix]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnvzRntcwUNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions[qix]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAKpiIAyrJKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "scores = []\n",
        "for qix, q in enumerate(questions):\n",
        "    qe = QE[qix]\n",
        "    \n",
        "    score = cosine_similarity([qe], CE)\n",
        "    ranking = score[0].argsort()[::-1]\n",
        "    result_ix = np.where(ranking == qix)\n",
        "    rix = result_ix[0][0]\n",
        "    scores.append( 1./(rix + 1) )\n",
        "\n",
        "    assert questions[ranking[rix]]['context'] == questions[qix]['context']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fzsFB4nwr2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZRmRzw1xT5y",
        "colab_type": "text"
      },
      "source": [
        "# Experiment 3 \n",
        "Use a Sentence Encoder to generete the embeddings.\n",
        "\n",
        "https://tfhub.dev/google/universal-sentence-encoder-lite/2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccfqyxTTzE9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install sentencepiece\n",
        "import sentencepiece as spm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gt-S6Vtzeof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C16qBvHG5j8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_to_IDs_in_sparse_format(sp, sentences):\n",
        "  # An utility method that processes sentences with the sentence piece processor\n",
        "  # 'sp' and returns the results in tf.SparseTensor-similar format:\n",
        "  # (values, indices, dense_shape)\n",
        "  ids = [sp.EncodeAsIds(x) for x in sentences]\n",
        "  max_len = max(len(x) for x in ids)\n",
        "  dense_shape=(len(ids), max_len)\n",
        "  values=[item for sublist in ids for item in sublist]\n",
        "  indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
        "  return (values, indices, dense_shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEG2fyenzXv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")\n",
        "  spm_path = sess.run(module(signature=\"spm_path\"))\n",
        "  # spm_path now contains a path to the SentencePiece model stored inside the\n",
        "  # TF-Hub module\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQuIWOuD6lD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_HGtMoS4rX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_embeddings(sentences):  \n",
        "    with tf.Session() as session:\n",
        "        input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
        "\n",
        "        module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")\n",
        "        \n",
        "        spm_path = session.run(module(signature=\"spm_path\"))\n",
        "        \n",
        "        sp = spm.SentencePieceProcessor()\n",
        "        sp.Load(spm_path)\n",
        "    \n",
        "        embeddings = module(\n",
        "            inputs=dict(\n",
        "                values=input_placeholder.values,\n",
        "                indices=input_placeholder.indices,\n",
        "                dense_shape=input_placeholder.dense_shape))\n",
        "        \n",
        "        values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, sentences)\n",
        "        # initialize\n",
        "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])        \n",
        "        # compute\n",
        "        message_embeddings = session.run(\n",
        "            embeddings,\n",
        "            feed_dict={input_placeholder.values: values,\n",
        "                        input_placeholder.indices: indices,\n",
        "                        input_placeholder.dense_shape: dense_shape})\n",
        "    return message_embeddings "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAdz0bFL6M8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_list = [ q['context'] for q in questions]\n",
        "query_list = [ q['question'] for q in questions] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma4NgB8r55J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CE = compute_embeddings(context_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CvYA7OO8RAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CE.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjFVoUDO6R_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "QE = compute_embeddings(query_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSlmXS3p5_IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "scores = []\n",
        "for qix, q in enumerate(questions):\n",
        "    qe = QE[qix]\n",
        "    \n",
        "    score = cosine_similarity([qe], CE)\n",
        "    ranking = score[0].argsort()[::-1]\n",
        "    result_ix = np.where(ranking == qix)\n",
        "    rix = result_ix[0][0]\n",
        "    scores.append( 1./(rix + 1) )\n",
        "\n",
        "    assert questions[ranking[rix]]['context'] == questions[qix]['context']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC4QghXC8aew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byd3167VzZhQ",
        "colab_type": "text"
      },
      "source": [
        "# Check Vocabolary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxiRHOLI0M1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp {MODEL_LOCATION}vocab.txt ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS9P0t3y0SBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 10 vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiCWX3jk0q7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_table('vocab.txt', names=['WORD'], encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2grMJjbI0miD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qjPf_Li4z8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 5028 vocab.txt | tail -n 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcnKR12h3xfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQpEZUAQ2jDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('vocab.txt') as fd:\n",
        "    vocab = set(list(l.strip() for l in fd.readlines()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDxkyPS93E0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.tokenize(\"don't be so judgmental\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULeyF6DS3gIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52b8o-Eq17o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for q in questions:\n",
        "    tokens = tokenizer.tokenize(q['question'])\n",
        "    found = 0\n",
        "    for word in tokens:\n",
        "        unique_words.append(word)\n",
        "        if word in vocab:\n",
        "            found += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI2iPu613oHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'judgment' in vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN6joVcG3lgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "found"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDMxM4El3cd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "found /  len(set(unique_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThlwX--10y_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 2500 vocab.txt | tail -n 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qaTRfAO047L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}