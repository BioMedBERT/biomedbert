{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E08w9hc0TAbI"
   },
   "source": [
    "# BioMedBERT BigQuery Data Analysis/ Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmQTSz_7Sy30"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I348xIulUEnV"
   },
   "outputs": [],
   "source": [
    "project_id = 'ai-vs-covid19'\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gL4YG5N6Tn61"
   },
   "source": [
    "## Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HvHzfvtKT6c9",
    "outputId": "c2e55ef1-3886-49b5-e4d0-2be6083d4926"
   },
   "outputs": [],
   "source": [
    "# Get number of rows\n",
    "row_count = client.query('''\n",
    "  SELECT \n",
    "    COUNT(*) as total\n",
    "  FROM `ai-vs-covid19.BigBioMedBERT2.ncbi_comm_use`''').to_dataframe().total\n",
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7urYHJj7UZXX",
    "outputId": "08e96a86-7ac0-42ea-97c3-ba93f9684512"
   },
   "outputs": [],
   "source": [
    "# get column names\n",
    "col_names = client.query('''\n",
    "  SELECT column_name\n",
    "  FROM `ai-vs-covid19.BigBioMedBERT2`.INFORMATION_SCHEMA.COLUMNS\n",
    "  WHERE table_name = 'ncbi_comm_use'\n",
    "''').to_dataframe()\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "8nEHxJIhVUOl",
    "outputId": "4408561d-0ef4-4305-c4e1-181205b32193"
   },
   "outputs": [],
   "source": [
    "# get general table information schema\n",
    "table_schema = client.query('''\n",
    "  SELECT *\n",
    "  FROM `ai-vs-covid19.BigBioMedBERT2`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\n",
    "  WHERE table_name = 'ncbi_comm_use'\n",
    "''').to_dataframe()\n",
    "table_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "colab_type": "code",
    "id": "eDhjuF80V8pl",
    "outputId": "a223c11e-1201-403b-820f-4fd1adaa5f15"
   },
   "outputs": [],
   "source": [
    "# select first 10 rows\n",
    "first_10_rows = client.query('''\n",
    "  SELECT *\n",
    "  FROM `ai-vs-covid19.BigBioMedBERT2.ncbi_comm_use`\n",
    "  LIMIT 10\n",
    "''').to_dataframe()\n",
    "first_10_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtA3hlOcWpsW"
   },
   "outputs": [],
   "source": [
    "# print body text\n",
    "def print_body(body_series: pd.Series) -> str:\n",
    "  print(textwrap.fill(body_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "0up3UTnLW1kA",
    "outputId": "1d7af912-bc5b-4af1-90fd-088254208795"
   },
   "outputs": [],
   "source": [
    "print_body(first_10_rows.Body[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6qIv0aCfioQ"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "zo9iWkzrfzM9",
    "outputId": "d21cbd0c-aecc-4e4c-8c54-f6c2dc6122c9"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "# !git clone https://github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_k54abHhhGP3"
   },
   "outputs": [],
   "source": [
    "# base imports\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "XOj-PvpHd0SC",
    "outputId": "bc809857-8cde-4017-aec4-926b446848aa"
   },
   "outputs": [],
   "source": [
    "#import bert modules\n",
    "sys.path.append(\"bert\")\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0Vw0rZIhsqI"
   },
   "source": [
    "### Preprocess text\n",
    "Remove pun—Åtuation, uppercase letters and non-utf symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLnaz54Pg13C"
   },
   "outputs": [],
   "source": [
    "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "  # lowercase text\n",
    "  text = str(text).lower()\n",
    "  # remove non-UTF\n",
    "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
    "  # remove punctuation symbols\n",
    "  text = \" \".join(regex_tokenizer.tokenize(text))\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vJ9M2-aYiAXb",
    "outputId": "b462ab03-011e-44ff-e743-0a62c58729fa"
   },
   "outputs": [],
   "source": [
    "# example to normalize text from ``Body`` in BioMedBERT dataset\n",
    "print_body(normalize_text(first_10_rows.Body[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk8rgWTLGoXT"
   },
   "source": [
    "## Create Expanded csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOiLn33dPPbl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbunQH1_SWWc"
   },
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tpbvp2ASAjC"
   },
   "outputs": [],
   "source": [
    "bucket=storage_client.get_bucket('big_bio_med_bert_dump_csv')\n",
    "# List all objects that satisfy the filter.\n",
    "blobs=bucket.list_blobs(prefix='ncbi_comm_use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pAaD8geSAmS"
   },
   "outputs": [],
   "source": [
    "blob = [blob for blob in blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WHy-ysLfnozM",
    "outputId": "aff1f76f-db54-4112-82ae-869c601603c7"
   },
   "outputs": [],
   "source": [
    "print(len(blob))\n",
    "print(len(blob) // 5)\n",
    "print((len(blob) // 5)*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f9pcw0ZhTas1"
   },
   "outputs": [],
   "source": [
    "# Create a function called \"chunks\" with two arguments, l and n:\n",
    "def split_list(data, chunk):\n",
    "    # For item i in a range that is a length of data (l),\n",
    "    for i in range(0, len(data), chunk):\n",
    "        # Create an index range for data of chunk (e.g. 5) items:\n",
    "        yield data[i:i+chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oi-tMaehoGZf"
   },
   "outputs": [],
   "source": [
    "# list of length in which we have to split \n",
    "blob_split_1, blob_split_2, blob_split_3, blob_split_4, blob_split_5 = list(split_list(blob, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "NMr8VEj8TbG9",
    "outputId": "ba4d3303-a2ad-4552-bae6-5dbc628d6ada"
   },
   "outputs": [],
   "source": [
    "print(len(blob))\n",
    "print(len(blob_split_1))\n",
    "print(len(blob_split_2))\n",
    "print(len(blob_split_3))\n",
    "print(len(blob_split_4))\n",
    "print(len(blob_split_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "-MakXh7v31xU",
    "outputId": "631dac94-aad5-4927-860b-765a3dc29695"
   },
   "outputs": [],
   "source": [
    "blob_split_1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "7eqOehVw4Lhh",
    "outputId": "2efffffb-8d4f-40a7-db40-a6a300770e51"
   },
   "outputs": [],
   "source": [
    "blob_split_2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-dXTAMgTur6"
   },
   "outputs": [],
   "source": [
    "def download_to_local(folder, blob_lst):\n",
    "    print('File download Started‚Ä¶. Wait for the job to complete.')\n",
    "    # Create this folder locally if not exists\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    # Iterating through for loop one by one using API call\n",
    "    for blob in blob_lst:\n",
    "#         print('Blobs: {}'.format(blob.name))\n",
    "        destination_uri = '{}/{}'.format(folder, (blob.name).split('/')[-1])\n",
    "        blob.download_to_filename(destination_uri)\n",
    "        print('Exported {} to {}'.format(blob.name, destination_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-CWG7n41_5x"
   },
   "outputs": [],
   "source": [
    "# !rm -rf data #data_1 data_2 data_3 data_4 #data_5\n",
    "# !rm ncbi_comm_use_csv_A.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S-luesxCT4uT",
    "outputId": "9ab6d42e-49c5-4fd2-abcf-38fd510e3ea3"
   },
   "outputs": [],
   "source": [
    "# download first part of csv's\n",
    "# download_to_local('data', blob)\n",
    "# download_to_local('data_1', blob_split_1)\n",
    "# download_to_local('data_2', blob_split_2)\n",
    "# download_to_local('data_3', blob_split_3)\n",
    "# download_to_local('data_4', blob_split_4)\n",
    "# download_to_local('data_5', blob_split_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96PJ6YMXT4xY"
   },
   "outputs": [],
   "source": [
    "# make combined csv\n",
    "def combined_csv(data_folder):\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob('{}/*.{}'.format(data_folder, extension))]\n",
    "    #combine all files in the list\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "    return combined_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdBDgQhnUrsp"
   },
   "outputs": [],
   "source": [
    "# blob_csv_A = combined_csv('data')\n",
    "# blob_csv = combined_csv('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dBVxTfO2Urqm",
    "outputId": "92f57f11-69db-493d-b367-099115c2ee0b"
   },
   "outputs": [],
   "source": [
    "# len(blob_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FX9RsVjKbc_Z"
   },
   "outputs": [],
   "source": [
    "# blob_csv.to_csv('gs://ekaba-assets/ncbi_comm_use.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9udoWp5weDq0"
   },
   "outputs": [],
   "source": [
    "# blob_csv_A.to_csv( \"ncbi_comm_use_csv_A.csv\", index=False, encoding='utf-8-sig')\n",
    "# blob_csv.to_csv( \"ncbi_comm_use.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsbTPI7aGmQs"
   },
   "outputs": [],
   "source": [
    "# copy files from gcs bucket\n",
    "# !gsutil -m cp gs://ekaba-assets/ncbi_comm_use_BODY.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ad8s14h8QL4d"
   },
   "outputs": [],
   "source": [
    "# body = pd.read_csv('ncbi_comm_use.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_sel = body[['Body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_sel.to_csv('gs://ekaba-assets/ncbi_comm_use_BODY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_sel.to_csv( \"ncbi_comm_use_BODY.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove FULL ncbi_comm_use.csv\n",
    "# !rm ncbi_comm_use.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRTgOh2UQL0M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOWtq2VJq2m7"
   },
   "outputs": [],
   "source": [
    "# convert csv to txt\n",
    "import csv\n",
    "import sys\n",
    "maxInt = sys.maxsize\n",
    "csv.field_size_limit(maxInt)\n",
    "\n",
    "csv_file = 'ncbi_comm_use_BODY.csv'\n",
    "txt_file = 'ncbi_comm_use_BODY.txt'\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move text file to GCS\n",
    "# !gsutil -m cp ncbi_comm_use_BODY.txt gs://ekaba-assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove csv file\n",
    "# !rm ncbi_comm_use_BODY.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xAPAIXtspSk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar\n",
    "def count_lines(filename):\n",
    "  count = 0\n",
    "  with open(filename) as fi:\n",
    "    for line in fi:\n",
    "      count += 1\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lnyso68uBp8r",
    "outputId": "a23ada36-710c-4099-b07b-145e7e03aa19"
   },
   "outputs": [],
   "source": [
    "# Apply normalization to entire dataset\n",
    "RAW_DATA_FPATH = \"ncbi_comm_use_BODY.txt\"\n",
    "PRC_DATA_FPATH = \"processed_ncbi_comm_use_BODY.txt\"\n",
    "\n",
    "# apply normalization to the dataset\n",
    "\n",
    "total_lines = count_lines(RAW_DATA_FPATH)\n",
    "bar = Progbar(total_lines)\n",
    "\n",
    "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
    "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
    "    for l in fi:\n",
    "      fo.write(normalize_text(l)+\"\\n\")\n",
    "      bar.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move processed text file to GCS\n",
    "# !gsutil -m cp processed_ncbi_comm_use_BODY.txt gs://ekaba-assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove intermediate files\n",
    "# !rm ncbi_comm_use_BODY.csv ncbi_comm_use_BODY.txt #processed_ncbi_comm_use_BODY.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3QSqf2hLpk6d"
   },
   "source": [
    "## Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRC_DATA_FPATH = \"processed_ncbi_comm_use_BODY.txt\"\n",
    "MODEL_PREFIX = \"biomedbert\" #@param {type: \"string\"}\n",
    "VOC_SIZE = 32000 #@param {type:\"integer\"}\n",
    "SUBSAMPLE_SIZE = 12800000 #@param {type:\"integer\"}\n",
    "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
    "\n",
    "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
    "               '--vocab_size={} --input_sentence_size={} '\n",
    "               '--shuffle_input_sentence=true ' \n",
    "               '--bos_id=-1 --eos_id=-1').format(\n",
    "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
    "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TUgPWa78sDjx",
    "outputId": "946a4f79-56d7-4182-896a-214bd95fa0e0"
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0zMTm7QJsdvQ",
    "outputId": "2e5ba2d6-1e11-4733-98e1-abc64d942953"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "91Vla2Ivx2S4",
    "outputId": "f0c7ee08-7b06-4f41-ff5d-9249ed0294ca"
   },
   "outputs": [],
   "source": [
    "!head -n 30 tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2skaOTxux5Xq"
   },
   "outputs": [],
   "source": [
    "def read_sentencepiece_vocab(filepath):\n",
    "  voc = []\n",
    "  with open(filepath, encoding='utf-8') as fi:\n",
    "    for line in fi:\n",
    "      voc.append(line.split(\"\\t\")[0])\n",
    "  # skip the first <unk> token\n",
    "  voc = voc[1:]\n",
    "  return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2u0i7LeJAmnO",
    "outputId": "3195d499-ff7a-4453-c41d-1bc51f685c0b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
    "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
    "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCW9uiQMAw1G"
   },
   "outputs": [],
   "source": [
    "def parse_sentencepiece_token(token):\n",
    "    if token.startswith(\"‚ñÅ\"):\n",
    "        return token[1:]\n",
    "    else:\n",
    "        return \"##\" + token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cq3ArCOvA4kV"
   },
   "outputs": [],
   "source": [
    "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hy-jY1pQA7aO"
   },
   "outputs": [],
   "source": [
    "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
    "bert_vocab = ctrl_symbols + bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gdq1IFoGBEAq",
    "outputId": "984065a5-f5f1-4afe-8102-0983161bc16f"
   },
   "outputs": [],
   "source": [
    "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
    "print(len(bert_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQHKGBCYBF5_"
   },
   "outputs": [],
   "source": [
    "# write vocabulary to file\n",
    "VOC_FNAME = \"vocab.txt\"\n",
    "\n",
    "with open(VOC_FNAME, \"w\") as fo:\n",
    "  for token in bert_vocab:\n",
    "    fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "ScifdT3eBPUH",
    "outputId": "93b746f4-8683-4892-9f60-e840cce8694d"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
    "bert_tokenizer.tokenize(first_10_rows.Body[0])[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DF5eYR9d0ZuL"
   },
   "source": [
    "## Generating pre-trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZixF84ZD0YH7",
    "outputId": "26d46f14-67c5-4919-ee7e-152572802e2f"
   },
   "outputs": [],
   "source": [
    "# sharding the dataset\n",
    "!mkdir ./shards\n",
    "!split -a 4 -l 5560 -d $PRC_DATA_FPATH ./shards/shard_\n",
    "!ls ./shards/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yedcP1p1JMF"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
    "PROCESSES = 2 #@param {type:\"integer\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9b555cSS1c0b"
   },
   "source": [
    "For each shard we need to call `create_pretraining_data.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5x2g476x1TJ-"
   },
   "outputs": [],
   "source": [
    "XARGS_CMD = (\"ls ./shards/ | \"\n",
    "             \"xargs -n 1 -P {} -I{} \"\n",
    "             \"python3 bert/create_pretraining_data.py \"\n",
    "             \"--input_file=./shards/{} \"\n",
    "             \"--output_file={}/{}.tfrecord \"\n",
    "             \"--vocab_file={} \"\n",
    "             \"--do_lower_case={} \"\n",
    "             \"--max_predictions_per_seq={} \"\n",
    "             \"--max_seq_length={} \"\n",
    "             \"--masked_lm_prob={} \"\n",
    "             \"--random_seed=34 \"\n",
    "             \"--dupe_factor=5\")\n",
    "\n",
    "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
    "                             VOC_FNAME, DO_LOWER_CASE, \n",
    "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eBei5rgb1jn3",
    "outputId": "0a76a538-ed96-4229-e678-259b82df0876"
   },
   "outputs": [],
   "source": [
    "tf.gfile.MkDir(PRETRAINING_DIR)\n",
    "!$XARGS_CMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r gs://ekaba-assets/pre_trained_data ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uz-18dZ2tDH"
   },
   "source": [
    "Save model assets and checkpoints to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMZKDdyL1pb3"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"ekaba-assets\"\n",
    "MODEL_DIR = \"bert_model\"\n",
    "tf.io.gfile.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mVR9qBK3V5j"
   },
   "source": [
    "Hyparameter configuration for BERT BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_SIZE = 32000\n",
    "VOC_FNAME = \"biomedbert-8M.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIibj7MY3TH5"
   },
   "outputs": [],
   "source": [
    "# use this for BERT-base\n",
    "\n",
    "bert_base_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1, \n",
    "  \"directionality\": \"bidi\", \n",
    "  \"hidden_act\": \"gelu\", \n",
    "  \"hidden_dropout_prob\": 0.1, \n",
    "  \"hidden_size\": 768, \n",
    "  \"initializer_range\": 0.02, \n",
    "  \"intermediate_size\": 3072, \n",
    "  \"max_position_embeddings\": 512, \n",
    "  \"num_attention_heads\": 12, \n",
    "  \"num_hidden_layers\": 12, \n",
    "  \"pooler_fc_size\": 768, \n",
    "  \"pooler_num_attention_heads\": 12, \n",
    "  \"pooler_num_fc_layers\": 3, \n",
    "  \"pooler_size_per_head\": 128, \n",
    "  \"pooler_type\": \"first_token_transform\", \n",
    "  \"type_vocab_size\": 2, \n",
    "  \"vocab_size\": VOC_SIZE\n",
    "}\n",
    "\n",
    "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
    "    json.dump(bert_base_config, fo, indent=2)\n",
    "  \n",
    "# with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
    "#   for token in bert_vocab:\n",
    "#     fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r $MODEL_DIR gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2QHoAJs63PrO",
    "outputId": "c98a8bcb-9ba0-43d1-9fe7-4df5f80419df"
   },
   "outputs": [],
   "source": [
    "# if BUCKET_NAME:\n",
    "#   !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VptZ5CEx3wWB"
   },
   "source": [
    "# Train the BioMedBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud auth application-default login --no-launch-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user tensorflow==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bert modules\n",
    "sys.path.append(\"bert\")\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR5ffWV15OHf"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# configure logging\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "u0fRU1XT3vu-",
    "outputId": "268c16ab-9274-4d9a-e185-b46b556dfd00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using checkpoint: None\n",
      "INFO:tensorflow:Using 10000 data shards\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = \"ekaba-assets\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
    "PRETRAINING_DIR = \"pre_trained_data\" #@param {type:\"string\"}\n",
    "VOC_FNAME = \"biomedbert-8M.txt\" #@param {type:\"string\"}\n",
    "\n",
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "if BUCKET_NAME:\n",
    "    BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
    "else:\n",
    "    BUCKET_PATH = \".\"\n",
    "\n",
    "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
    "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
    "\n",
    "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
    "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.io.gfile.glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
    "log.info(\"Using {} data shards\".format(len(input_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEawhTlo5frp"
   },
   "source": [
    "**Train on TPUs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "513l2s0g5nOx",
    "outputId": "9ab147b4-2d1f-49a9-aee3-b552de014b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using TPU runtime\n"
     ]
    }
   ],
   "source": [
    "if 'TPU_NAME' in os.environ:\n",
    "    log.info(\"Using TPU runtime\")\n",
    "    USE_TPU = True\n",
    "    TPU_ADDRESS = 'grpc://' + '10.250.1.2:8470'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "aM4Vn5RZ3pqk",
    "outputId": "b96576de-fb9c-47a2-b770-572e99dba93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f2e55e97ea0>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_save_checkpoints_secs': None, '_experimental_max_worker_delay_secs': None, '_keep_checkpoint_max': 5, '_session_creation_timeout_secs': 7200, '_master': 'grpc://10.250.1.2:8470', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2e55aef978>, '_save_summary_steps': 100, '_global_id_in_cluster': 0, '_protocol': None, '_experimental_distribute': None, '_log_step_count_steps': None, '_eval_distribute': None, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f2e55aef160>, '_model_dir': 'gs://ekaba-assets/bert_model', '_evaluation_master': 'grpc://10.250.1.2:8470', '_num_ps_replicas': 0, '_train_distribute': None, '_device_fn': None, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_tf_random_seed': None, '_task_type': 'worker', '_service': None, '_session_config': allow_soft_placement: true\n",
      "cluster_def {\n",
      "  job {\n",
      "    name: \"biomedbert\"\n",
      "    tasks {\n",
      "      key: 0\n",
      "      value: \"10.250.1.2:8470\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "isolate_session_state: true\n",
      ", '_save_checkpoints_steps': 2500, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "tpu_cluster_resolver =  tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "    tpu=TPU_ADDRESS, zone='us-central1-a', project='ai-vs-covid19', job_name='biomedbert')\n",
    "\n",
    "run_config = tf.compat.v1.estimator.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=BERT_GCS_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n",
    "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  \n",
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-3KYolcy5kvn",
    "outputId": "03a57f14-eba6-4e78-9325-63677b8cb253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.250.1.2:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:CPU:0, CPU, -1, 9577806028284405766)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2943474117103428884)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5351861045237112242)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10694017606547298861)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6358258477034075477)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1904455500127653085)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU:5, TPU, 17179869184, 16282173350475439365)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17177720372664859542)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17922846540146991802)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 10910863515283077968)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:biomedbert/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 13431747664116294373)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
      "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
      "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
      "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
      "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
      "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
      "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
      "INFO:tensorflow:Found small feature: next_sentence_labels [16, 1]\n",
      "INFO:tensorflow:*** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (16, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (16, 128)\n",
      "INFO:tensorflow:  name = masked_lm_ids, shape = (16, 20)\n",
      "INFO:tensorflow:  name = masked_lm_positions, shape = (16, 20)\n",
      "INFO:tensorflow:  name = masked_lm_weights, shape = (16, 20)\n",
      "INFO:tensorflow:  name = next_sentence_labels, shape = (16, 1)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (16, 128)\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (32000, 768)\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
      "INFO:tensorflow:  name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
      "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
      "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
      "INFO:tensorflow:  name = cls/predictions/output_bias:0, shape = (32000,)\n",
      "INFO:tensorflow:  name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
      "INFO:tensorflow:  name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:TPU job name biomedbert\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "ERROR:tensorflow:Error recorded from training_loop: Error executing an HTTP request: HTTP response code 403 with body '{\n",
      "  \"error\": {\n",
      "    \"code\": 403,\n",
      "    \"message\": \"Insufficient Permission\",\n",
      "    \"errors\": [\n",
      "      {\n",
      "        \"message\": \"Insufficient Permission\",\n",
      "        \"domain\": \"global\",\n",
      "        \"reason\": \"insufficientPermissions\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "'\n",
      "\t when initiating an upload to gs://ekaba-assets/bert_model/graph.pbtxt.tmp524db6a594ab44fbbd3c2ae492fb3c99\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "WARNING:tensorflow:Reraising captured error\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error executing an HTTP request: HTTP response code 403 with body '{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"Insufficient Permission\",\n    \"errors\": [\n      {\n        \"message\": \"Insufficient Permission\",\n        \"domain\": \"global\",\n        \"reason\": \"insufficientPermissions\"\n      }\n    ]\n  }\n}\n'\n\t when initiating an upload to gs://ekaba-assets/bert_model/graph.pbtxt.tmp524db6a594ab44fbbd3c2ae492fb3c99",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fc1a0ca3c649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   3028\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m           saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m   3031\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1193\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1194\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_creation_timeout_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m         log_step_count_steps=log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[1;32m   1491\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    723\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \"\"\"\n\u001b[1;32m   1206\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         logging.info(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m       \u001b[0;31m# Inform the hooks that a new session has been created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m       return _CoordinatedSession(\n\u001b[1;32m    887\u001b[0m           \u001b[0m_HookedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36mafter_create_session\u001b[0;34m(self, session, coord)\u001b[0m\n\u001b[1;32m    570\u001b[0m     training_util.write_graph(\n\u001b[1;32m    571\u001b[0m         \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         self._checkpoint_dir, \"graph.pbtxt\")\n\u001b[0m\u001b[1;32m    573\u001b[0m     \u001b[0msaver_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     70\u001b[0m     file_io.atomic_write_string_to_file(path,\n\u001b[1;32m     71\u001b[0m                                         text_format.MessageToString(\n\u001b[0;32m---> 72\u001b[0;31m                                             graph_def, float_format=''))\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[0;34m(filename, contents, overwrite)\u001b[0m\n\u001b[1;32m    536\u001b[0m   \"\"\"\n\u001b[1;32m    537\u001b[0m   \u001b[0mtemp_pathname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tmp\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m   \u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwrite_string_to_file\u001b[0;34m(filename, file_content)\u001b[0m\n\u001b[1;32m    345\u001b[0m   \"\"\"\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;34m\"\"\"Make usable with \"with\" statement.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mret_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSet_TF_Status_from_Status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    557\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: Error executing an HTTP request: HTTP response code 403 with body '{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"Insufficient Permission\",\n    \"errors\": [\n      {\n        \"message\": \"Insufficient Permission\",\n        \"domain\": \"global\",\n        \"reason\": \"insufficientPermissions\"\n      }\n    ]\n  }\n}\n'\n\t when initiating an upload to gs://ekaba-assets/bert_model/graph.pbtxt.tmp524db6a594ab44fbbd3c2ae492fb3c99"
     ]
    }
   ],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KajSEMOYm50O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BioMedBERT-Data-Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
